<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>bpmNEXT 2019 impressions, day 3</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RoZjc5DxAbA/bpmnext-2019-impressions-day-3.html" /><category term="bpmNEXT" scheme="searchisko:content:tags" /><category term="Demo" scheme="searchisko:content:tags" /><category term="feed_group_name_jbpm" scheme="searchisko:content:tags" /><category term="feed_name_kverlaen" scheme="searchisko:content:tags" /><category term="Presentation" scheme="searchisko:content:tags" /><author><name>Kris Verlaenen</name></author><id>searchisko:content:id:jbossorg_blog-bpmnext_2019_impressions_day_3</id><updated>2019-04-24T10:52:55Z</updated><published>2019-04-24T10:52:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div style="text-align: justify;"&gt;Last (half) day where I have to present myself as well (as 3rd of the day)&lt;b&gt;.&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;A Well-Mixed Cocktail: Blending Decision and RPA Technologies in 1st Gen Design Patterns&lt;/b&gt;&lt;br /&gt;&lt;i&gt;Lloyd Dugan&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Lloyd introduced an RPA-enabled case mgmt platform that is used in the context of a use case to determine eligibility for Affordable Care Act. Using Sapiens for decisions and Appian for BPM, approximately 4000 people are using this as a work mgmt application (where work is assigned to people so they can work through this).&amp;nbsp; To be able to achieve higher throughput, they however combined this with RPA that emulate the behavoir of the users.&amp;nbsp; He showed (unfortunately in a prerecorded video, not a live demo) how they implemented the robots to perform some of the work (up to 50% of the total work done by the users !). The robots learned how to soft fail if there were issues (in which case the work would go back into the queue), needed to accomodate for latency, etc. &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Emergent Synthetic Process&lt;/b&gt;&lt;br /&gt;&lt;i&gt;Keith Swenson - Fujitsu&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Keith presented a way to customize processes to different contexts (for example slightly different regulations / approaches in different countries) by being able to generate a customized process for your specific context (when you start the process).&amp;nbsp; Rather than encoding processes in a procedural manner (after A do B), he is using "service descriptions" to define the tasks and the preconditions. You can then generate a process from this by specifying your goal and context and working backwards to create a customized process from this.&amp;nbsp; This allows you to add new tasks to these processes easily (as this is much more declarative logic and therefore additive).&lt;br /&gt;The demo showed a travel application with approval by different people. Service descriptions can have required tasks, required data, etc.&amp;nbsp; The process is generated by working backwards from the goal, adding required steps one by one.&amp;nbsp; Different countries can add their own steps, leading to small customizations in the generated process. &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Automating Human-Centric Processes with Machine Learning&lt;/b&gt;&lt;br /&gt;&lt;i&gt;Kris Verlaenen - Red Hat&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;I was up next !&amp;nbsp; I presented on how to combine Process Automation and Machine Learning (ML), to create a platform that combines the benefits of encoding business logic using a combination of business processes, rules etc. but at the same time can become more intelligent over time by observing and learning from the data during execution.&amp;nbsp; The focus was on introducing "non-intrusive" ways of combining processes with ML, to assist users with performing their tasks rather than to try and replace them.&lt;br /&gt;The demo was using the it-orders application (one of our out-of-the-box case management demos that employees can use to order laptops) that focused on 3 main use cases:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Augmenting task data&lt;/i&gt;:&amp;nbsp; While human actors are performing tasks in your processes or cases, we can observe the data and try to predict task outcomes based on task inputs.&amp;nbsp; Once the ML algorithm (using Random Forest algorithm, with the SMILE library as the implementation) has been trained a little, it can start augmenting the data with possible predictions, but also with a confidence it has on that prediction, the relative importance of the input parameters, etc.&amp;nbsp; In this case, the manager approving the order would be able to see this augmented data in his task form and use it to make the right decision.&lt;/li&gt;&lt;li&gt;&lt;i&gt;Recommending tasks&lt;/i&gt;:&amp;nbsp; Case management allows users to add addition dynamic tasks to running cases (even though they weren't modeled in the case upfront) in specific situations.&amp;nbsp; Similarly, these can be monitored and ML could be used to detect patterns.&amp;nbsp; These could be turned into recommendations, where a user is presented with a recommendation to do (or assign) a task based on what the ML algorithm has learned.&amp;nbsp; This can help the users significantly to not forget things or to assist them by preparing most of the work (they simply have to accept the recommendation).&lt;/li&gt;&lt;li&gt;&lt;i&gt;Optimizing processes based on ML&lt;/i&gt;: One of the advantages of the Random Forest algorithm is that you can inspect the decision trees that are being trained to see what they have learned so far.&amp;nbsp; Since ML also has disadvantages (that it can be biased or that it is simply learning from what is being done, which is not necessarily correct behavior), analyzing what was learned so far and integrating this back into the process (and/or rules etc.) has significant advantages as well.&amp;nbsp;&amp;nbsp; We extended the existing case with additional logic (like for example an additional decision service to determine whether some manager approvals could be automated, or additional ad-hoc tasks included in the case that would be triggered under certain circumstances), so that some of the patterns detected by ML would be encoded and enforced by the case logic itself.&lt;/li&gt;&lt;/ul&gt;These non-introsive ways of combining processes with ML is very complementary (as it allows us to take advantage of both approaches which mitigates some of the disadvantages of ML) and allows users to start getting advantages of ML and build up confidence in small and incremental steps.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;b&gt;ML, Conversational UX, and Intelligence in BPM&lt;/b&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;i&gt;Andre Hofeditz, Seshadri Sreeniva - SAP SE&lt;/i&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;SAP is presenting "live processes" that are created by combining predefined building blocks, running on their platform with support for conversational user experience, decision management, task inbox, etc.&amp;nbsp;&amp;nbsp;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;SAP API Business Hub has been extended to also include live processes. Using an employee onboarding scenario, they show how a running instance can be "configured" (only in specific situations, which you can define during authoring) after which you can change the template and generate a new variant.&amp;nbsp; The process visibility workbench allows to generate a customizable UI for monitoring progress of your processes.&lt;/div&gt;&lt;div style="text-align: justify;"&gt;Next, they show how you can extend the platform by using recipes, which can be imported in SAP web IDE and deployed into the platform, adding additional capabilities that will be available in your live processes from that point forward.&lt;/div&gt;&lt;div style="text-align: justify;"&gt;Finally, they showed an intelligent assistant that is a sort of chatbot that can respond to voice.&amp;nbsp; It can give an aggregated view of your tasks, complete the tasks through the conversational UI, etc.&amp;nbsp; They showed how the chatbot can be programmed by defining tasks with triggers, requirements and actions, which can then be deployed as a microservice on the SAP cloud.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;DMN TCK&lt;/b&gt;&lt;br /&gt;&lt;i&gt;Keith Swenson&amp;nbsp;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Keith&lt;i&gt; &lt;/i&gt;explained the efforts that are going into the DMN TCK, a set of tests to verify the compliance of DMN engines.&amp;nbsp; When running these tests, it takes a large number of models and test cases (currently over a thousand but still growing) and check the results.&amp;nbsp; He explained some of the challenges and opportunities in this context (e.g. error handling).&lt;br /&gt;While many vendors claim DMN compatibility, Red Hat is one of the few vendors that actually has the results to prove it !&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;That concludes bpmNEXT 2019!&amp;nbsp; As previous years, I very much enjoyed the presentations, but probably even more the discussions during the breakouts and evenings.&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RoZjc5DxAbA" height="1" width="1" alt=""/&gt;</content><summary>Last (half) day where I have to present myself as well (as 3rd of the day). A Well-Mixed Cocktail: Blending Decision and RPA Technologies in 1st Gen Design Patterns Lloyd Dugan Lloyd introduced an RPA-enabled case mgmt platform that is used in the context of a use case to determine eligibility for Affordable Care Act. Using Sapiens for decisions and Appian for BPM, approximately 4000 people are us...</summary><dc:creator>Kris Verlaenen</dc:creator><dc:date>2019-04-24T10:52:00Z</dc:date><feedburner:origLink>http://kverlaen.blogspot.com/2019/04/bpmnext-2019-impressions-day-3.html</feedburner:origLink></entry><entry><title>How to run systemd in a container</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5QRBMS2wE74/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Podman" scheme="searchisko:content:tags" /><category term="systemd" scheme="searchisko:content:tags" /><author><name>Daniel Walsh</name></author><id>searchisko:content:id:jbossorg_blog-how_to_run_systemd_in_a_container</id><updated>2019-04-24T07:02:10Z</updated><published>2019-04-24T07:02:10Z</published><content type="html">&lt;p&gt;I have been talking about &lt;a href="https://github.com/systemd/systemd"&gt;systemd&lt;/a&gt; in a container for a long time. Way back in 2014, I wrote “&lt;a href="https://developers.redhat.com/blog/2014/05/05/running-systemd-within-docker-container/"&gt;Running systemd within a Docker Container&lt;/a&gt;.” And, a couple of years later, I wrote another article, “&lt;a href="https://developers.redhat.com/blog/2016/09/13/running-systemd-in-a-non-privileged-container/"&gt;Running systemd in a non-privileged container&lt;/a&gt;,” explaining how things hadn’t gotten much better. In that article, I stated, “Sadly, two years later if you google Docker systemd, this is still the article people see—it’s time for an update.” I also linked to a talk I about &lt;a href="https://lwn.net/Articles/676831/"&gt;how upstream Docker and upstream systemd would not compromise.&lt;/a&gt; In this article, I&amp;#8217;ll look at the progress that&amp;#8217;s been made and how Podman can help. &lt;span id="more-589467"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;There are lots of reasons to run systemd inside a system, such as:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;b&gt;Multiservice containers&lt;/b&gt;—Lots of people want to take existing multi-service applications out of VMs and run them inside of containers. We would prefer that they break apart these applications into microservices, but some people can’t or don’t have time yet.  So running them as services launched out of unit files by systemd makes sense.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Systemd unit files&lt;/b&gt;—Most applications that run inside of containers are built from code that was run in VMs or on host systems. These applications have a unit file that was written for the application and understands how to run the application. It can be better to launch the service via the supported method, rather than to hack up your own init service.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Systemd is a process manager&lt;/b&gt;—It handles the management of services like reaping, restarting, and shutting down better than any other tool.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That being said, there are also lots of reasons not to run systemd in containers. The main one is that systemd/journald controls the output of containers, whereas tools like &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://www.openshift.com/"&gt;OpenShift&lt;/a&gt; expect the containers to log directly to stdout and stderr. So, if you are going to manage your containers via Orchestrator like these, then you should think twice about using systemd-based containers. Additionally, the upstream community of Docker and Moby were often hostile to the use of systemd in a container.&lt;/p&gt; &lt;h2&gt;Enter Podman&lt;/h2&gt; &lt;p&gt;I am happy to say things have gotten better. My team, container runtimes, at Red Hat decided to build &lt;a href="https://podman.io/"&gt;our own container engine&lt;/a&gt;, called &lt;a href="https://github.com/containers/libpod"&gt;Podman&lt;/a&gt;. Podman is a container engine with the same command-line interface (CLI) as Docker. Pretty much every command you can run from the Docker command line you can execute with &lt;a href="https://developers.redhat.com/blog/tag/podman/"&gt;Podman&lt;/a&gt;. I often give a talk now called &lt;a href="https://podman.io/talks/2018/10/01/talk-replace-docker-with-podman.html"&gt;Replacing Docker with Podman&lt;/a&gt;, where the first slide says &lt;code&gt;alias docker=podman&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;And lots of people had.&lt;/p&gt; &lt;p&gt;With Podman, however, we were not hostile to systemd-based containers. Systemd is the most prevalent Linux init system on the planet, and not allowing it to run properly within a container would ignore the way thousands of users choose to run containers.&lt;/p&gt; &lt;p&gt;Podman understands what systemd needs to do to run in a container. It requires things like tmpfs mounted at /run and /tmp. It likes to have the “container” environment turned on, and it expects to be able to write to its portion of the cgroup directory and to the /var/log/journald directory.&lt;/p&gt; &lt;p&gt;When Podman starts a container that is running init or systemd as its initial command, Podman automatically sets up the tmpfs and Cgroups for systemd to start without a problem. If you want to block the systemd behavior, you have to run &lt;code&gt;--systemd=false&lt;/code&gt;. Note that the systemd behavior only happens when Podman sees the command to be executed is systemd or init.&lt;/p&gt; &lt;p&gt;Here is the man page description:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;man podman run&lt;/p&gt; &lt;p&gt;…&lt;/p&gt; &lt;p&gt;&amp;#8211;systemd=true|false&lt;/p&gt; &lt;p&gt;Run container in systemd mode. The default is true.&lt;/p&gt; &lt;p&gt;If the command you running inside of the container is systemd or init, podman will setup tmpfs mount points in the following directories:&lt;/p&gt; &lt;p&gt;/run, /run/lock, /tmp, /sys/fs/cgroup/systemd, /var/lib/journal&lt;/p&gt; &lt;p&gt;It will also set the default stop signal to SIGRTMIN+3.&lt;/p&gt; &lt;p&gt;This allows systemd to run in a confined container without any modifications.&lt;/p&gt; &lt;p&gt;Note: On SELinux systems, systemd attempts to write to the cgroup file system.  Containers writing to the cgroup file system are denied by default. The container_manage_cgroup boolean must be enabled for this to be allowed on an SELinux separated system.&lt;/p&gt; &lt;p&gt;setsebool -P container_manage_cgroup true&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Now let’s look at a Dockerfile for running systemd in a container using Podman:&lt;/p&gt; &lt;pre&gt;# cat Dockerfile FROM fedora RUN dnf -y install httpd; dnf clean all; systemctl enable httpd EXPOSE 80 CMD [ "/sbin/init" ] &lt;/pre&gt; &lt;p&gt;That’s it.&lt;/p&gt; &lt;p&gt;Build the container&lt;/p&gt; &lt;pre&gt;# podman build -t systemd .&lt;/pre&gt; &lt;p&gt;Tell SELinux it is ok to allow systemd to manipulate its Cgroups configuration.&lt;/p&gt; &lt;pre&gt;# setsebool -P container_manage_cgroup true&lt;/pre&gt; &lt;p&gt;You will forget to do this; I did while writing this blog. Luckily, you do this once, and it will be set for the lifetime of the system.&lt;/p&gt; &lt;p&gt;Now just run the container.&lt;/p&gt; &lt;pre&gt;# podman run -ti -p 80:80 systemd systemd 239 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=hybrid) Detected virtualization container-other. Detected architecture x86-64. Welcome to Fedora 29 (Container Image)! Set hostname to &amp;#60;1b51b684bc99&amp;#62;. Failed to install release agent, ignoring: Read-only file system File /usr/lib/systemd/system/systemd-journald.service:26 configures an IP firewall (IPAddressDeny=any), but the local system does not support BPF/cgroup based firewalling. Proceeding WITHOUT firewalling in effect! (This warning is only shown for the first loaded unit using IP firewalling.) [  OK ] Listening on initctl Compatibility Named Pipe. [  OK ] Listening on Journal Socket (/dev/log). [  OK ] Started Forward Password Requests to Wall Directory Watch. [  OK ] Started Dispatch Password Requests to Console Directory Watch. [  OK ] Reached target Slices. … [  OK ] Started The Apache HTTP Server. &lt;/pre&gt; &lt;p&gt;And the service is up and running.&lt;/p&gt; &lt;pre&gt;$ curl localhost &amp;#60;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"&amp;#62; … &amp;#60;/html&amp;#62; &lt;/pre&gt; &lt;p&gt;Note: Don’t try this with Docker you still need to jump through hoops to get a container like this running in the daemon. (You need additional fields and packages, to make this work seamlessly in Docker, or run in a &amp;#8211;privileged container. &lt;a href="https://developers.redhat.com/blog/2016/09/13/running-systemd-in-a-non-privileged-container/"&gt;My previous article&lt;/a&gt; explains this better.)&lt;/p&gt; &lt;h2&gt;Other cool features about Podman and systemd&lt;/h2&gt; &lt;h3&gt;Podman in systemd unit files works better than Docker&lt;/h3&gt; &lt;p&gt;When launching containers at boot, you can simply put Podman commands into a systemd unit file, and systemd will launch and monitor the service. Podman is a standard fork and exec model. That means the container processes are children of the Podman process, so systemd has an easy time monitoring the processes.&lt;/p&gt; &lt;p&gt;Docker is a client service model and putting the Docker CLI into a unit file is possible. However, as the Docker client connects to the Docker daemon, the Docker client becomes just another process handling stdin and stdout. Systemd has no idea of this relationship between the Docker client and the container that is running under the Docker daemon and can&amp;#8217;t monitor the service in this model.&lt;/p&gt; &lt;h3&gt;Systemd socket activation&lt;/h3&gt; &lt;p&gt;Podman works correctly when the socket is activated. Because Podman is a fork/exec model, it can pass the connected socket down to its children container processes. Docker cannot do this because of the client/server model.&lt;/p&gt; &lt;p&gt;Podman &lt;a href="https://varlink.org/"&gt;varlink&lt;/a&gt;, a service that Podman uses for remote clients to interact with containers, is actually socket activated. The &lt;a href="https://github.com/cockpit-project/cockpit-podman"&gt;cockpit-podman&lt;/a&gt; package, written in Node.js, is part of the cockpit project and allows people to interact with Podman containers via a web interface. The web daemon running cockpit-podman sends messages to a varlink socket that systemd is listening on. Systemd then activates the Podman program to receive the messages and start managing containers. Systemd socket activation allows us to have no long-running daemon and still be able to handle a remote API.&lt;/p&gt; &lt;p&gt;We are developing another client for Podman, called&lt;em&gt; podman-remote&lt;/em&gt;, which implements the same Podman CLI but calls into varlink to launch containers. Podman-remote can work over SSH sessions, allowing us to securely interact with containers on different machines. We eventually plan on using podman-remote to support MacOS and Windows users as well as Linux users. This will allow developers on a Mac or Windows box to launch a Linux VM with Podman varlink running and have the feeling that containers are running on their local machine.&lt;/p&gt; &lt;h3&gt;SD_NOTIFY&lt;/h3&gt; &lt;p&gt;Systemd has the ability to hold up secondary services from starting that rely on a containerized service starting. Podman can pass down the SD_NOTIFY Socket to the containerized service, so it can notify systemd when it is ready to begin servicing requests. Docker again cannot do this, because of the client/server model.&lt;/p&gt; &lt;h2&gt;Future Work&lt;/h2&gt; &lt;p&gt;We have plans to add a &lt;code&gt;podman generate systemd CONTAINERID&lt;/code&gt;, which would generate a systemd unit file for managing the specified container. This should work in either root or rootless mode for non-privileged containers. I have even seen a PR to create a systemd-nspawn OCI-compliant runtime.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Running systemd in a container is a reasonable thing to do. Finally, we have a container runtime in Podman that is not hostile to running systemd fully but easily enables the workload.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#38;linkname=How%20to%20run%20systemd%20in%20a%20container" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fhow-to-run-systemd-in-a-container%2F&amp;#038;title=How%20to%20run%20systemd%20in%20a%20container" data-a2a-url="https://developers.redhat.com/blog/2019/04/24/how-to-run-systemd-in-a-container/" data-a2a-title="How to run systemd in a container"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/24/how-to-run-systemd-in-a-container/"&gt;How to run systemd in a container&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5QRBMS2wE74" height="1" width="1" alt=""/&gt;</content><summary>I have been talking about systemd in a container for a long time. Way back in 2014, I wrote “Running systemd within a Docker Container.” And, a couple of years later, I wrote another article, “Running systemd in a non-privileged container,” explaining how things hadn’t gotten much better. In that article, I stated, “Sadly, two years later if you google Docker systemd, this is still the article peo...</summary><dc:creator>Daniel Walsh</dc:creator><dc:date>2019-04-24T07:02:10Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/24/how-to-run-systemd-in-a-container/</feedburner:origLink></entry><entry><title>Using Quiver with AMQ on Red Hat OpenShift Container Platform</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/l7ZAenG0X2k/" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="jboss a-mq" scheme="searchisko:content:tags" /><category term="OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="performance" scheme="searchisko:content:tags" /><category term="Quiver" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>gahealy</name></author><id>searchisko:content:id:jbossorg_blog-using_quiver_with_amq_on_red_hat_openshift_container_platform</id><updated>2019-04-24T07:00:57Z</updated><published>2019-04-24T07:00:57Z</published><content type="html">&lt;p&gt;As part of the Red Hat &lt;a href="https://www.redhat.com/en/services/consulting"&gt;UKI Professional Services&lt;/a&gt; team, I have worked with several customers who are implementing &lt;a href="https://developers.redhat.com/products/amq/overview/"&gt;AMQ Broker&lt;/a&gt; on &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform (OCP)&lt;/a&gt;. One question customers typically ask is, “How do we validate that the AMQ configuration is correct for our scenario?” Previously, I would have suggested one of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="http://activemq.apache.org/activemq-performance-module-users-manual.html"&gt;ActiveMQ Perf Maven plugin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://activemq.apache.org/jmeter-performance-tests.html"&gt;JMeter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://gatling.io/docs/3.0/jms/"&gt;Gatling&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These tools can give you indicators around:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the broker up and running? That is, can it receive/publish messages for this configuration?&lt;/li&gt; &lt;li&gt;Can the broker handle a certain performance characteristic? That is, what is my minimum publish rate per second for this configuration?&lt;/li&gt; &lt;li&gt;And much more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The problem with these tools is that you cannot choose the client technology. This could lead to real-world differences and limited technology choices, which in turn might lead you down the wrong technology path. In other words:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you get the same performance from JMeter versus the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html/amq_clients_overview/components"&gt;AMQ clients&lt;/a&gt; you would use in production? Are you comparing like for like? Apples with apples?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, what do I think is the answer? &lt;a href="https://github.com/ssorj/quiver"&gt;Quiver &lt;/a&gt;&lt;a href="#DISCLAIMER"&gt;[1]&lt;/a&gt;. In this article, I&amp;#8217;ll provide an overview and demo of using Quiver with Red Hat AMQ on Red Hat OpenShift.  If you&amp;#8217;re looking for more information on Red Hat AMQ and how it can help, check out this &lt;a href="https://youtu.be/mkqVxWZfGfI"&gt;webinar.&lt;/a&gt;&lt;span id="more-588787"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2 id="what-is-quiver"&gt;What is Quiver?&lt;/h2&gt; &lt;p&gt;Straight from the &lt;a href="https://github.com/ssorj/quiver"&gt;Quiver GitHub page&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;Quiver implementations are native clients (and sometimes also servers) in various languages and APIs that either send or receive messages and write raw information about the transfers to standard output. They are deliberately simple.&lt;/p&gt;&lt;/blockquote&gt; &lt;blockquote&gt;&lt;p&gt;The quiver-arrow command runs a single implementation in send or receive mode and captures its output. It has options for defining the execution parameters, selecting the implementation, and reporting statistics.&lt;/p&gt;&lt;/blockquote&gt; &lt;blockquote&gt;&lt;p&gt;The quiver command launches a pair of quiver-arrow instances, one sender and one receiver, and produces a summary of the end-to-end transmission of messages.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;In my own words;&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;Quiver is a tool that starts both a publisher and receiver client with the aim of completing as fast as possible. Once the publisher and receiver have completed; a statistical output is generated which can give you insight into how the test has performed.&lt;/p&gt;&lt;/blockquote&gt; &lt;blockquote&gt;&lt;p&gt;The clients are based on the &lt;a href="https://qpid.apache.org/"&gt;Apache Qpid&lt;/a&gt; project that offers; Java, C++, Python and Javascript implementations to name a few. The clients primarily target &lt;a href="https://download.oracle.com/otndocs/jcp/jms-2_0-fr-eval-spec/"&gt;JMS&lt;/a&gt; and &lt;a href="http://www.amqp.org/resources/download"&gt;AMQP&lt;/a&gt; users.&lt;/p&gt;&lt;/blockquote&gt; &lt;h2 id="recorded-demo"&gt;Recorded demo&lt;/h2&gt; &lt;p&gt;Feeling lazy and want to watch an &lt;a href="https://asciinema.org/"&gt;asciinema&lt;/a&gt; recording?&lt;/p&gt; &lt;p&gt;The demo will show the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Broker and AMQ Interconnect being deployed onto OKD running on Minishift&lt;/li&gt; &lt;li&gt;Send and receive messages via Quiver using the Core protocol to the broker&lt;/li&gt; &lt;li&gt;Send and receive messages via Quiver using a JMS client over AMQP protocol to the broker&lt;/li&gt; &lt;li&gt;Send and receive messages via Quiver using a JMS client over AMQP protocol to interconnect&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://asciinema.org/a/240989"&gt;&lt;img src="https://asciinema.org/a/240989.png" alt="asciicast" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2 id="do-it-yourself-demo"&gt;Do it yourself demo&lt;/h2&gt; &lt;p&gt;Want to play with Quiver, AMQ, and OCP yourself?&lt;/p&gt; &lt;p&gt;The demo uses an OCP template which requires two arguments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DOCKER_IMAGE; location of the fully qualified docker pull URL. This can be resolved via the imported image stream; &lt;code&gt;oc get is quiver -o jsonpath=‘{.status.dockerImageRepository}’&lt;/code&gt;&lt;/li&gt; &lt;li&gt;DOCKER_CMD; the quiver command, in JSON array format, you want to execute.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id="demo-prerequisites"&gt;Demo prerequisites&lt;/h3&gt; &lt;p&gt;It is presumed that you have the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Basic knowledge of the OCP CLI and web console&lt;/li&gt; &lt;li&gt;A running &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform (OCP)&lt;/a&gt; or &lt;a href="https://github.com/minishift/minishift"&gt;Minishift&lt;/a&gt; environment&lt;/li&gt; &lt;li&gt;AMQ Broker templates and image streams installed by following this documentation: &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/deploying_amq_broker_on_openshift_container_platform/index#install-deploy-ocp-broker-ocp"&gt;2.1. Installing AMQ Broker on OpenShift Container Platform image streams and application templates&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AMQ Interconnect templates and image streams installed by following: &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html/deploying_amq_interconnect_on_openshift_container_platform/preparing-to-deploy-router-ocp"&gt;2.1. Verifying the availability of AMQ Interconnect templates&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3 id="setup"&gt;Setup&lt;/h3&gt; &lt;ol type="1"&gt; &lt;li&gt;Create project &lt;pre&gt; $ oc new-project quiver&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Import the Quiver image &lt;pre&gt; $ oc import-image quiver:latest --from=docker.io/ssorj/quiver --confirm&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Add the view role to the default service account &lt;pre&gt; $ oc policy add-role-to-user view system:serviceaccount:$(oc project -q):default&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy AMQ Broker &lt;pre&gt; $ oc new-app --template=amq-broker-72-basic \ -e AMQ_PROTOCOL=openwire,amqp,stomp,mqtt \ -e AMQ_QUEUES=quiver \ -e AMQ_ADDRESSES=quiver \ -e AMQ_USER=anonymous \ -e ADMIN_PASSWORD=password $ oc get pods --watch&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy AMQ Interconnect &lt;pre&gt; $ oc new-app --template=amq-interconnect-1-basic $ oc get pods --watch&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id="send-messages-to-amq-broker"&gt;Send messages to AMQ Broker&lt;/h3&gt; &lt;ol start="6" type="1"&gt; &lt;li&gt;Send 1,000,000 messages to the broker via the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/using_the_amq_core_protocol_jms_client/"&gt;core protocol&lt;/a&gt; &lt;pre&gt; $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/activemq-artemis-jms\", \"--impl\", \"activemq-artemis-jms\", \"--verbose\"]" \ | oc create -f -&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Look at output for core protocol pod &lt;pre&gt; $ oc get pods --watch $ oc logs -f {insert value of new pod name} ---------------------- Sender ----------------------- --------------------- Receiver ---------------------- -------- Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Lat [ms] ----------------------------------------------------- ----------------------------------------------------- -------- 2.3 0 0 73 64.6 2.3 0 0 73 63.8 0 4.3 3,337 1,667 71 100.0 4.3 2,529 1,261 75 96.6 42 6.3 24,727 10,684 71 116.0 6.4 25,178 11,223 106 123.1 36 8.3 113,170 44,177 64 116.7 8.4 116,495 45,658 66 123.9 2 10.3 221,942 54,359 59 116.8 10.4 227,809 55,491 50 124.0 2 12.3 327,983 52,994 56 116.8 12.4 332,016 52,077 44 122.3 2 14.3 428,866 50,391 54 116.8 14.4 433,091 50,512 41 122.5 2 16.3 540,823 55,923 62 117.1 16.4 547,056 56,926 49 122.6 2 18.3 645,499 52,312 60 117.1 18.4 648,975 50,934 48 122.6 2 20.3 761,704 58,073 58 117.1 20.4 768,000 59,453 55 122.7 2 22.3 873,509 55,875 61 117.2 22.4 874,857 53,348 50 122.7 3 24.3 914,014 20,242 33 117.2 24.5 923,768 22,728 27 122.7 4 26.3 1,000,000 42,950 53 117.4 26.5 1,000,000 38,078 45 0.0 2 -------------------------------------------------------------------------------- Subject: activemq-artemis-jms //broker-amq-tcp:61616/activemq-artemis-jms (/tmp/quiver-xo6l0q4a) Count: 1,000,000 messages Body size: 100 bytes Credit window: 1,000 messages Duration: 23.0 s Sender rate: 43,414 messages/s Receiver rate: 43,507 messages/s End-to-end rate: 43,407 messages/s Latencies by percentile: 0%: 0 ms 90.00%: 4 ms 25%: 2 ms 99.00%: 20 ms 50%: 2 ms 99.90%: 249 ms 100%: 307 ms 99.99%: 290 ms --------------------------------------------------------------------------------&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Send 1,000,000 messages to the broker via the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/using_the_amq_jms_client/"&gt;JMS client over AMQP protocol&lt;/a&gt; &lt;pre&gt; $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"amqp://broker-amq-amqp:5672/qpid-jms-over-ampq\", \"--impl\", \"qpid-jms\", \"--verbose\"]" \ | oc create -f -&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Look at output for the AMQP client pod &lt;pre&gt; $ oc get pods --watch $ oc logs -f {insert value of new pod name} ---------------------- Sender ----------------------- --------------------- Receiver ---------------------- -------- Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Lat [ms] ----------------------------------------------------- ----------------------------------------------------- -------- 2.2 2,663 1,330 109 103.1 2.2 1,984 991 94 94.6 89 4.2 16,304 6,817 118 110.7 4.2 16,113 7,050 115 113.2 56 6.2 45,720 14,693 102 119.7 6.2 45,809 14,767 97 115.4 118 8.2 105,097 29,674 88 120.8 8.2 92,810 23,489 84 115.6 305 10.2 159,018 26,920 71 120.8 10.2 146,602 26,883 76 115.6 411 12.2 213,794 27,361 81 121.5 12.2 205,564 29,466 81 117.2 400 14.2 271,138 28,643 77 121.6 14.2 256,132 25,271 76 117.2 461 16.2 325,058 26,960 82 116.6 16.2 311,757 27,785 75 117.2 566 18.2 383,747 29,315 78 116.6 18.2 362,224 25,208 72 117.2 678 20.2 431,554 23,880 64 116.6 20.2 411,578 24,665 70 117.2 750 22.2 492,444 30,430 75 116.6 22.2 468,113 28,225 77 117.2 786 24.2 537,684 22,609 70 116.7 24.2 510,287 21,087 70 117.3 1,105 26.2 595,639 28,963 77 116.7 26.2 567,833 28,759 75 117.4 1,049 28.2 646,014 25,175 67 116.7 28.2 619,412 25,790 73 117.5 1,014 30.2 701,401 27,680 73 116.9 30.2 669,677 25,095 72 117.5 1,167 32.2 757,523 28,019 73 116.9 32.2 722,166 26,231 72 117.6 1,246 34.2 803,007 22,731 70 117.1 34.2 768,587 23,210 66 117.6 1,360 36.2 856,193 26,593 69 117.1 36.2 816,627 23,924 73 117.6 1,538 38.2 911,459 27,605 73 117.1 38.2 870,633 26,990 77 117.6 1,633 40.2 968,803 28,658 70 117.4 40.2 926,359 27,849 76 117.6 1,482 ---------------------- Sender ----------------------- --------------------- Receiver ---------------------- -------- Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Lat [ms] ----------------------------------------------------- ----------------------------------------------------- -------- 42.2 1,000,000 15,583 41 0.0 42.2 978,444 26,029 89 117.6 1,581 - - - - - 44.2 1,000,000 10,773 57 0.0 1,608 -------------------------------------------------------------------------------- Subject: qpid-jms amqp://broker-amq-amqp:5672/qpid-jms-over-ampq (/tmp/quiver-a1xz3nbz) Count: 1,000,000 messages Body size: 100 bytes Credit window: 1,000 messages Duration: 42.4 s Sender rate: 24,696 messages/s Receiver rate: 23,616 messages/s End-to-end rate: 23,573 messages/s Latencies by percentile: 0%: 1 ms 90.00%: 1575 ms 25%: 534 ms 99.00%: 1741 ms 50%: 988 ms 99.90%: 1886 ms 100%: 1929 ms 99.99%: 1926 ms --------------------------------------------------------------------------------&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id="send-messages-to-amq-interconnect"&gt;Send messages to AMQ Interconnect&lt;/h3&gt; &lt;ol start="10" type="1"&gt; &lt;li&gt;Send 1,000,000 messages to the interconnect via the JMS client over AMQP protocol &lt;pre&gt;$ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"amqp://amq-interconnect:5672/qpid-jms-over-ampq-via-interconnect\", \"--impl\", \"qpid-jms\", \"--verbose\"]" \ | oc create -f -&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Look at output for AMQP client pod &lt;pre&gt;$ oc get pods --watch $ oc logs -f {insert value of new pod name} ---------------------- Sender ----------------------- --------------------- Receiver ---------------------- -------- Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Lat [ms] ----------------------------------------------------- ----------------------------------------------------- -------- 2.2 1,529 764 131 102.6 2.2 1,465 731 120 84.5 21 4.2 12,580 5,514 151 116.2 4.2 12,529 5,526 138 114.5 16 6.2 42,121 14,756 125 119.5 6.2 41,815 14,606 90 111.3 6 8.2 85,563 21,721 82 119.9 8.2 85,028 21,596 71 111.7 4 10.2 125,149 19,773 75 120.2 10.2 124,858 19,905 62 112.0 5 12.2 169,899 22,264 79 116.5 12.2 169,965 22,486 67 112.1 5 14.3 212,816 21,437 69 116.7 14.2 212,442 21,217 60 112.1 5 16.3 259,889 23,536 75 116.7 16.2 259,571 23,553 71 114.7 4 18.3 308,674 24,332 86 119.1 18.2 308,723 24,539 69 114.9 4 20.3 340,342 15,826 67 119.1 20.2 340,176 15,726 58 115.0 6 22.3 386,804 23,208 76 119.3 22.2 386,597 23,199 67 115.0 4 24.3 434,611 23,880 75 119.3 24.2 434,435 23,895 65 115.0 4 26.3 472,270 18,783 63 119.4 26.2 471,754 18,650 53 115.0 6 28.3 507,116 17,406 59 119.4 28.2 506,646 17,420 51 115.0 6 30.3 541,841 17,345 57 119.4 30.2 541,639 17,488 56 115.1 6 32.3 577,421 17,781 61 119.4 32.2 576,429 17,352 50 115.1 6 34.3 612,512 17,528 61 119.4 34.2 612,231 17,892 53 115.1 6 36.3 644,913 16,192 58 119.5 36.2 644,190 15,972 48 115.1 6 38.3 678,782 16,901 58 119.5 38.2 678,577 17,176 52 115.1 7 40.3 714,240 17,720 60 119.5 40.2 713,569 17,487 51 115.3 6 ---------------------- Sender ----------------------- --------------------- Receiver ---------------------- -------- Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Time [s] Count [m] Rate [m/s] CPU [%] RSS [M] Lat [ms] ----------------------------------------------------- ----------------------------------------------------- -------- 42.3 750,187 17,965 62 119.5 42.2 748,967 17,672 51 115.3 6 44.3 787,234 18,496 63 119.5 44.3 787,500 19,171 55 115.3 6 46.3 820,124 16,420 58 119.5 46.3 819,155 15,820 48 115.3 6 48.3 856,316 18,087 59 119.5 48.3 855,868 18,338 51 115.3 6 50.3 897,520 20,551 68 119.5 50.3 897,232 20,641 57 115.3 5 52.3 944,594 23,537 73 119.5 52.3 944,159 23,452 63 115.3 4 54.3 990,567 22,986 72 119.5 54.3 990,176 22,997 64 115.3 4 56.3 1,000,000 4,716 17 0.0 56.3 1,000,000 4,907 15 0.0 4 -------------------------------------------------------------------------------- Subject: qpid-jms amqp://amq-interconnect:5672/qpid-jms-over-ampq-via-interconnect (/tmp/quiver-0sttx8_z) Count: 1,000,000 messages Body size: 100 bytes Credit window: 1,000 messages Duration: 53.7 s Sender rate: 18,615 messages/s Receiver rate: 18,634 messages/s End-to-end rate: 18,614 messages/s Latencies by percentile: 0%: 0 ms 90.00%: 10 ms 25%: 3 ms 99.00%: 18 ms 50%: 5 ms 99.90%: 43 ms 100%: 89 ms 99.99%: 72 ms --------------------------------------------------------------------------------&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id="cleanup"&gt;Cleanup&lt;/h3&gt; &lt;ol start="12" type="1"&gt; &lt;li&gt;Cleanup and delete all quiver pods &lt;pre&gt;$ oc delete pod -l app=quiver&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2 id="does-your-client-use-another-language"&gt;Does your client use another language?&lt;/h2&gt; &lt;p&gt;If your language of choice was not shown in the demo, the good news is that Quiver supports several &lt;a href="https://github.com/ssorj/quiver#quiver-1"&gt;implementations&lt;/a&gt;. Here are some examples:&lt;/p&gt; &lt;pre&gt;$ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/activemq-jms\", \"--impl\", \"activemq-jms\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/qpid-messaging-cpp\", \"--impl\", \"qpid-messaging-cpp\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/qpid-messaging-python\", \"--impl\", \"qpid-messaging-python\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/qpid-proton-c\", \"--impl\", \"qpid-proton-c\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/qpid-proton-cpp\", \"--impl\", \"qpid-proton-cpp\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/qpid-proton-python\", \"--impl\", \"qpid-proton-python\", \"--verbose\"]" \ | oc create -f - $ oc process -f https://raw.githubusercontent.com/ssorj/quiver/0.2.0/packaging/openshift/openshift-pod-template.yml \ DOCKER_IMAGE=$(oc get is quiver -o jsonpath='{.status.dockerImageRepository}'):latest \ DOCKER_CMD="[\"quiver\", \"//broker-amq-tcp:61616/rhea\", \"--impl\", \"rhea\", \"--verbose\"]" \ | oc create -f -&lt;/pre&gt; &lt;h2 id="food-for-thought"&gt;Food for thought&lt;/h2&gt; &lt;p&gt;I hope this demo has shown how easy it is to use Quiver to interact with AMQ Broker and AMQ Interconnect on OCP.&lt;/p&gt; &lt;p&gt;It should have also raised questions around how Quiver could be integrated into your own systems. For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Could Quiver be part of your mvn integration tests?&lt;/li&gt; &lt;li&gt;Could Quiver be added as a stage within your CI/CD pipeline for smoke testing?&lt;/li&gt; &lt;li&gt;Could the results of the smoke test pass/fail the deployment?&lt;/li&gt; &lt;li&gt;Could Quiver be used as part of a heartbeat monitoring system to validate that the broker is alive?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are several scenarios for which I see Quiver being highly useful, and you’ve probably thought of others.&lt;/p&gt; &lt;h3 id="disclaimer"&gt;&lt;a name="DISCLAIMER"&gt;&lt;/a&gt;Note&lt;/h3&gt; &lt;p&gt;[1] Although Quiver is developed by Red Hat employees, it is not supported under a Red Hat subscription and is strictly an upstream project.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#38;linkname=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F24%2Fusing-quiver-with-amq-on-red-hat-openshift-container-platform%2F&amp;#038;title=Using%20Quiver%20with%20AMQ%20on%20Red%20Hat%20OpenShift%20Container%20Platform" data-a2a-url="https://developers.redhat.com/blog/2019/04/24/using-quiver-with-amq-on-red-hat-openshift-container-platform/" data-a2a-title="Using Quiver with AMQ on Red Hat OpenShift Container Platform"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/24/using-quiver-with-amq-on-red-hat-openshift-container-platform/"&gt;Using Quiver with AMQ on Red Hat OpenShift Container Platform&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/l7ZAenG0X2k" height="1" width="1" alt=""/&gt;</content><summary>As part of the Red Hat UKI Professional Services team, I have worked with several customers who are implementing AMQ Broker on Red Hat OpenShift Container Platform (OCP). One question customers typically ask is, “How do we validate that the AMQ configuration is correct for our scenario?” Previously, I would have suggested one of the following: ActiveMQ Perf Maven plugin JMeter Gatling These tools ...</summary><dc:creator>gahealy</dc:creator><dc:date>2019-04-24T07:00:57Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/24/using-quiver-with-amq-on-red-hat-openshift-container-platform/</feedburner:origLink></entry><entry><title>Red Hat Summit 2019: IT Automation and Management Labs Roadmap</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Nns4oHb0b6A/red-hat-summit-2019-it-automation-management-labs-roadmap.html" /><category term="AppDev" scheme="searchisko:content:tags" /><category term="cloud" scheme="searchisko:content:tags" /><category term="conference" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_summit_2019_it_automation_and_management_labs_roadmap</id><updated>2019-04-24T05:00:05Z</updated><published>2019-04-24T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;a href="https://reg.summit.redhat.com/" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" target="_blank"&gt;&lt;img border="0" data-original-height="389" data-original-width="769" height="161" src="https://1.bp.blogspot.com/-yX0WaXZmaTI/XJj8CSvWUlI/AAAAAAAAth4/4Z1aZQ7VOp061sKNZ-_kMauutZPP46AMgCLcBGAs/s320/Screenshot%2B2019-03-25%2Bat%2B17.04.08.png" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://www.redhat.com/en/summit/2019" target="_blank"&gt;Red Hat Summit 2019&lt;/a&gt; is rocking Boston, MA from May 7-9th in the &lt;a href="https://www.signatureboston.com/BCEC" target="_blank"&gt;Boston Convention and Exhibition Center&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Everything you need to know about the current state of open source enterprise ready software can be found at this event. From customers talking about their experiences leveraging open source in their solutions, to the creators of open source technologies you're using, and all the way down to hands-on lab experiences on these technologies.&lt;br /&gt;&lt;br /&gt;This hands-on appeal is what this series of articles is about. It's interesting to take a tour, so starting with this article let's examine a series of instructor-led labs based on a specific theme.&lt;br /&gt;&lt;br /&gt;This week it's a roadmap to &lt;i&gt;IT automation and management&lt;/i&gt; lab content.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;The following labs can be found in the&amp;nbsp;&lt;a href="https://summit.redhat.com/conference/sessions" target="_blank"&gt;session catalog online&lt;/a&gt;, by searching on title or filtering on &lt;i&gt;instructor-led labs&lt;/i&gt; and &lt;i&gt;IT automation and management.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Ansible F5 automation workshop&lt;/h3&gt;&lt;i&gt;This is a hands-on lab from Red Hat and F5 Networks covering Red Hat Ansible Engine, Red Hat Ansible Tower, and F5 Networks BIG-IP. You'll get to automate and manage F5 BIG-IP nodes and build a web app on your own private workbench. Instructors will include network automation experts from F5 Networks and Red Hat Ansible. This session is geared toward those with limited or no experience in Ansible automation. The intended audience is any network engineer (various experience levels) or a system administrator with some very basic knowledge of F5 BIG-IP. New to F5 and just want to learn? No problem, this session will be beneficial to anyone willing to learn.&lt;br /&gt;&lt;br /&gt;Speakers: Colin McNaughton, Payal Singh, Sean Cavanaugh&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Deploy and scale Microsoft Azure infrastructures and applications with Red Hat Ansible Automation&lt;/h3&gt;&lt;i&gt;Immerse yourself in the world of Ansible by Red Hat and Microsoft Azure. Explore the vast Ansible module landscape for Azure and obtain hands-on experience deploying IaaS, PaaS and other infrastructures to Azure using Ansible playbooks. You'll receive the building blocks to extend your existing Ansible deployment to Azure and instructions on how to take advantage of both infrastructure and platform services. You'll perform the following entirely from Ansible Playbooks:&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Create a Red Hat Enterprise Linux virtual machine in Azure using the Azure Marketplace.&lt;/i&gt;&lt;/li&gt;&lt;i&gt;&lt;li&gt;Create and configure an Azure MySQL PaaS database.&lt;/li&gt;&lt;li&gt;Deploy an application on the Red Hat Enterprise Linux virtual machine which utilizes the Azure MySQL PaaS database.&lt;/li&gt;&lt;li&gt;Generalize the Red Hat Enterprise Linux virtual machine image to create a golden image template for group deployments.&lt;/li&gt;&lt;li&gt;Scale out the application to multiple servers using Azure virtual machine scale sets.&lt;/li&gt;&lt;li&gt;Create an application gateway &amp;amp; load balancer to front-end the deployed application.&lt;/li&gt;&lt;/i&gt;&lt;/ul&gt;&lt;i&gt;In addition, hands-on labs will be available to showcase:&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;ul&gt;&lt;i&gt;&lt;li&gt;Big data workloads using Azure HDInsight.&lt;/li&gt;&lt;li&gt;High-performance computing using Azure virtual machine infiniband interconnects.&lt;/li&gt;&lt;li&gt;Launching an application in Azure Kubernetes Service (AKS).&lt;/li&gt;&lt;li&gt;Serverless applications using Azure functions.&lt;/li&gt;&lt;/i&gt;&lt;/ul&gt;&lt;i&gt;You'll be provided with access to an Azure subscription and will not require any Azure / Microsoft credentials. All content will be obtained from GitHub and will persist.&lt;br /&gt;&lt;br /&gt;Speakers: Stuart Kirk, Zim Kalinowski, Harold Wong&lt;/i&gt;&lt;br /&gt;&lt;h3&gt;From source to RPM in 120 minutes&lt;/h3&gt;&lt;i&gt;In this lab, we'll learn best practices for packaging software using the Red Hat Enterprise Linux native packaging format, RPM. We'll cover how to properly build software from source code into RPM packages, create RPM packages from pre-compiled binaries, and to automate RPM builds from source code version control systems (such as Git) for use in CI/DevOps environments. And finally, we'll hear tips and tricks from lessons learned, such as how to set up and work with pristine build environments and why such things are important to software packaging.&lt;br /&gt;&lt;br /&gt;Speakers: Adam Miller, Carl George, Rob Marti, Tom Sorensen&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Getting started with Ansible&lt;/h3&gt;&lt;i&gt;Ansible is a simple yet powerful IT automation engine for app deployment, configuration management, and orchestration that you can learn quickly. In this lab, after a brief introduction, you'll install Red Hat Ansible Automation and run the first commands. Then, we'll tackle some of the basic concepts, and you'll start to write your first playbooks. Along the way, you'll learn more advanced concepts, such as controlling task execution and templating.&lt;br /&gt;&lt;br /&gt;Speakers: Eric Lavarde, Goetz Rieger, Roland Wolters, Daniel Brintzinger&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Getting started with Red Hat Ansible Tower&lt;/h3&gt;&lt;i&gt;Red Hat Ansible Tower helps you centralize and control your IT infrastructure with a visual dashboard, role-based access control, job scheduling, and graphical inventory management. In this lab you'll start with configuring inventories and credentials, then learn how to integrate your playbooks. After configuring job templates, you'll run your first jobs using Ansible Tower. Finally, we'll show you how to give users without Ansible knowledge limited control of playbook execution and introduce you to the workflows feature. This lab is best for people who already have basic experience with Ansible.&lt;br /&gt;&lt;br /&gt;Speaker(s): Eric Lavarde, Goetz Rieger, Daniel Brintzinger, Roland Wolters&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Hands on with Red Hat Satellite 6.5&lt;/h3&gt;&lt;i&gt;In this lab, you'll be exposed to the latest version of Red Hat Satellite, which is 6.5. You'll be able to exercise new features and see how Satellite improves the management of Red Hat Enterprise Linux.&lt;br /&gt;&lt;br /&gt;Speakers: Amir Feferkuchen, Bryan Kearney, John Mitsch, Peter Ondrejka&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Hands-on integrated management lab&lt;/h3&gt;&lt;i&gt;This hands-on lab will give you experience with full integration of all Red Hat Management products and services: Red Hat Satellite, Red Hat Ansible Tower, Red Hat Insights, Red Hat CloudForms, Red Hat CloudForms for hybrid cloud management and multicloud OS management. You'll be provided a working environment and be walked through integrations and major use case functionality with each product or service.&lt;br /&gt;&lt;br /&gt;Speakers: Andrés Valero, Chris Short, John Spinks, Roland Wolters, William Nix&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Implementing proactive security and compliance automation and DevSecOps&lt;/h3&gt;&lt;i&gt;In this hands-on lab, you’ll learn how to implement security and compliance automation for the infrastructure, operations, and application across a hybrid environment using a combination of various Red Hat products. Specifically, you’ll use a combination of Red Hat’s management and automation products, Red Hat OpenShift Container Platform, and OpenSCAP to:&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Perform automated audit scans to quickly detect and automatically remediate security and compliance issues in a controlled way to ensure compliance against regulatory or custom profiles and for automated configuration compliance.&lt;/i&gt;&lt;/li&gt;&lt;i&gt;&lt;li&gt;Implement automated web application hardening.&lt;/li&gt;&lt;li&gt;Automatically provision a security-compliant host.&lt;/li&gt;&lt;li&gt;Implement infrastructure, security, and compliance as code.&lt;/li&gt;&lt;li&gt;Implement consistent and automated patch and configuration management.&lt;/li&gt;&lt;li&gt;Proactively identify and remediate security threats at scale with predictive analytics.&lt;/li&gt;&lt;li&gt;Have centralized management of your hybrid infrastructure for continuous security and monitoring.&lt;/li&gt;&lt;li&gt;Build security into your application by implementing DevSecOps at scale using Red Hat OpenShift Container Platform and several other tools, such as OWASP ZAP, SonarQube, Clair, and more to build a secure CI/CD application pipeline.&lt;/li&gt;&lt;/i&gt;&lt;/ul&gt;&lt;i&gt;Speakers: Lucy Kerner, Justin Lacey, Kevin Holmes, Khary Mendez, Will Tome&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Red Hat Ansible Tower for advanced users&lt;/h3&gt;&lt;i&gt;Have you started using Red Hat Ansible Tower, already love it, and want to know more? Then this lab is for you.&amp;nbsp;&lt;/i&gt;&lt;i&gt;You know the basics, so we'll give you a hands-on introduction to the more advanced concepts and features. We'll start by learning and exploring Ansible Tower clustering and how to use it for high availability and load balancing. Next, we'll cover the isolated node feature, which allows you to automate hosts in separate networks with limited access. You already know about inventories, so we'll step it up a bit by introducing you to dynamic inventories and the smart inventory feature. Along the way, you'll learn how to organize your Ansible roles and content in well-structured Git repositories. And finally, we'll cover using the API to expose even more ways to take advantage of Ansible Tower's power.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;Speakers: Eric Lavarde, Goetz Rieger, Daniel Brintzinger, Roland Wolters&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Stay tuned for more articles with insights into other themes that might interest you enough to register for one of these instructor-led labs at Red Hat Summit 2019.&lt;br /&gt;&lt;br /&gt;Looking forward to seeing you there!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ZqUDndC2yO4:1QCsHPN63Qk:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ZqUDndC2yO4:1QCsHPN63Qk:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ZqUDndC2yO4:1QCsHPN63Qk:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=ZqUDndC2yO4:1QCsHPN63Qk:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=ZqUDndC2yO4:1QCsHPN63Qk:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/ZqUDndC2yO4" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Nns4oHb0b6A" height="1" width="1" alt=""/&gt;</content><summary>Red Hat Summit 2019 is rocking Boston, MA from May 7-9th in the Boston Convention and Exhibition Center. Everything you need to know about the current state of open source enterprise ready software can be found at this event. From customers talking about their experiences leveraging open source in their solutions, to the creators of open source technologies you're using, and all the way down to ha...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2019-04-24T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/ZqUDndC2yO4/red-hat-summit-2019-it-automation-management-labs-roadmap.html</feedburner:origLink></entry><entry><title>Keycloak Releases and Versioning</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/K_l34VVomts/versioning.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_releases_and_versioning</id><updated>2019-04-24T00:00:00Z</updated><published>2019-04-24T00:00:00Z</published><content type="html">&lt;p&gt; We are aiming to achieve a continuous delivery model with Keycloak. By that we mean it should be seamless to upgrade between Keycloak releases and to keep up to date with the latest release. &lt;/p&gt; &lt;p&gt; This requires no breaking changes, but rather deprecating old APIs allowing time to migrate to new APIs. &lt;/p&gt; &lt;p&gt; Traditional semantic versioning does not fit very well with this model. By following the mantra of continuous delivery we would forever be stuck on a major version and only update the minor version, and you could argue whether or not it would be correct to update the major version when an API that has been deprecated for a long period of time is removed. &lt;/p&gt; &lt;p&gt; With this in mind, we have made some slight changes to our release cadence and versioning schema. &lt;/p&gt; &lt;p&gt; For now, we will have a new feature release roughly 4 times each year. Each release will bump the major version number. That doesn't mean there are breaking changes, but until we perfect our continuous delivery model there may be some, so always refer to the migration guide prior to upgrading! &lt;/p&gt; &lt;p&gt; We have also decided to drop the Final suffix from releases. That is simply because it is not needed as we have not done any beta or release candidates for a long time. In the spirit of continuous delivery, we will have individual features marked as preview rather than whole releases. &lt;/p&gt; &lt;p&gt; As a final note, with the reduced release cadence we are planning to do more micro releases. This will be focused on critical bugs and security vulnerabilities. However, we may accept contributions to less critical bugs given the fix is well tested and has low risk of regressions. &lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/K_l34VVomts" height="1" width="1" alt=""/&gt;</content><summary>We are aiming to achieve a continuous delivery model with Keycloak. By that we mean it should be seamless to upgrade between Keycloak releases and to keep up to date with the latest release. This requires no breaking changes, but rather deprecating old APIs allowing time to migrate to new APIs. Traditional semantic versioning does not fit very well with this model. By following the mantra of conti...</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2019-04-24T00:00:00Z</dc:date><feedburner:origLink>https://www.keycloak.org/2019/04/versioning.html</feedburner:origLink></entry><entry><title>What is Apache Camel K - Awesome 30 minute video</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_VUa3xB5Upg/what-is-apache-camel-k-awesome-30.html" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="camelk" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_clausibsen" scheme="searchisko:content:tags" /><category term="video" scheme="searchisko:content:tags" /><author><name>Claus Ibsen</name></author><id>searchisko:content:id:jbossorg_blog-what_is_apache_camel_k_awesome_30_minute_video</id><updated>2019-04-23T11:14:35Z</updated><published>2019-04-23T11:14:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Last year we created the &lt;a href="https://github.com/apache/camel-k"&gt;Apache Camel K&lt;/a&gt; sub-project of &lt;a href="http://camel.apache.org/"&gt;Apache Camel&lt;/a&gt;. Since we have been busy working on Camel K and also the upcoming Apache Camel v3.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;What is Apache Camel K ?&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;i&gt;A lightweight integration platform born on #Kubernetes, with serverless superpowers &lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Last week Nicola and Luca gave a fantastic overview and demo of Apache Camel K. They talk was recorded and its posted online on &lt;a href="https://www.youtube.com/watch?v=51x9BewGCYA"&gt;youtube&lt;/a&gt;. If you have the time I highly recommend to watch the video and see where we are going with serverless integration with Camel K.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe width="320" height="266" class="YOUTUBE-iframe-video" data-thumbnail-src="https://i.ytimg.com/vi/51x9BewGCYA/0.jpg" src="https://www.youtube.com/embed/51x9BewGCYA?feature=player_embedded" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=-NUjxGuP1A4:_VnjL1jcBnQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=-NUjxGuP1A4:_VnjL1jcBnQ:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=-NUjxGuP1A4:_VnjL1jcBnQ:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=-NUjxGuP1A4:_VnjL1jcBnQ:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=-NUjxGuP1A4:_VnjL1jcBnQ:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=-NUjxGuP1A4:_VnjL1jcBnQ:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=-NUjxGuP1A4:_VnjL1jcBnQ:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/ApacheCamel/~4/-NUjxGuP1A4" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_VUa3xB5Upg" height="1" width="1" alt=""/&gt;</content><summary>Last year we created the Apache Camel K sub-project of Apache Camel. Since we have been busy working on Camel K and also the upcoming Apache Camel v3. What is Apache Camel K ? A lightweight integration platform born on #Kubernetes, with serverless superpowers Last week Nicola and Luca gave a fantastic overview and demo of Apache Camel K. They talk was recorded and its posted online on youtube. If ...</summary><dc:creator>Claus Ibsen</dc:creator><dc:date>2019-04-23T11:14:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/-NUjxGuP1A4/what-is-apache-camel-k-awesome-30.html</feedburner:origLink></entry><entry><title>Red Hat OpenShift 4.0 Developer Preview on AWS: Up and running with Windows</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/i3rZ2OnLi4Y/" /><category term="aws" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="developer preview" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="OpenShift4" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><author><name>Don Schenck</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_openshift_4_0_developer_preview_on_aws_up_and_running_with_windows</id><updated>2019-04-23T07:03:07Z</updated><published>2019-04-23T07:03:07Z</published><content type="html">&lt;p&gt;In a previous article, &lt;a href="https://developers.redhat.com/blog/2019/03/07/openshift-4-0-developer-preview-on-aws-is-up-and-running/"&gt;&amp;#8220;OpenShift 4.0 Developer Preview on AWS is up and running&amp;#8221;&lt;/a&gt; I included instructions for using macOS or Linux to install and manage your Red Hat OpenShift 4.0 cluster. Since I recently added a Windows 10 PC to my technology mix, I decided to try to use Windows as my only choice.&lt;/p&gt; &lt;p&gt;I was saddened to learn that the installer, &lt;code&gt;openshift-install&lt;/code&gt;, isn&amp;#8217;t available for Windows. But, like any developer who won&amp;#8217;t be denied; I found a way.&lt;span id="more-586327"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The key to success is to embrace the &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10"&gt;Windows Subsystem for Linux&lt;/a&gt; (WSL). With this tool in hand, you can download and install the necessary bits to start up your Red Hat OpenShift 4.0 cluster on AWS, then switch over to PowerShell on Windows to log in and do your development. In my case, my code is written in C# and, although the WSL Linux that I used (Fedora Remix) does have vi installed, I want to do my work in Visual Studio Code. Windows is my comfort zone; why should I be forced to leave it? Besides, I know how to exit VS Code (**grin**).&lt;/p&gt; &lt;h2&gt;Overview&lt;/h2&gt; &lt;p&gt;Here is the plan, the TL;DR:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Enable WSL (Windows).&lt;/li&gt; &lt;li&gt;Install a Linux Distro (Windows).&lt;/li&gt; &lt;li&gt;Configure AWS (Linux).&lt;/li&gt; &lt;li&gt;Install openshift-install (Linux).&lt;/li&gt; &lt;li&gt;Start the cluster (Linux).&lt;/li&gt; &lt;li&gt;Install the OpenShift CLI for Windows (Windows).&lt;/li&gt; &lt;li&gt;Copy the configuration file from WSL to Windows (Both).&lt;/li&gt; &lt;li&gt;Log in from Windows (Windows).&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;But first&amp;#8230;&lt;/h2&gt; &lt;p&gt;I strongly recommend reading the aforementioned article, &lt;a href="https://developers.redhat.com/blog/2019/03/07/openshift-4-0-developer-preview-on-aws-is-up-and-running/"&gt;&amp;#8220;OpenShift 4.0 Developer Preview on AWS is up and running&amp;#8221;&lt;/a&gt;, for some details that are not repeated here. Don&amp;#8217;t worry if you don&amp;#8217;t understand all the Linux commands; the important part is to know what steps need to happen.&lt;/p&gt; &lt;h2&gt;Enable WSL&lt;/h2&gt; &lt;p&gt;The PowerShell command is:&lt;/p&gt; &lt;pre&gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux&lt;/pre&gt; &lt;h2&gt;Install a Linux distro&lt;/h2&gt; &lt;p&gt;Go to the Microsoft Store on your Windows PC and install the Linux distro of your choosing. For this article, I spent the five bucks and opted for the &amp;#8220;Fedora Remix&amp;#8221; distro. If you install a different distro, the package-related commands will differ (e.g., &amp;#8220;yum&amp;#8221; for Fedora versus &amp;#8220;apt-get&amp;#8221; for Ubuntu). The rest will be the same, however.&lt;/p&gt; &lt;p&gt;Install the distro and open a terminal window.&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-586747 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/04/distro-terminal.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/04/distro-terminal.png" alt="" width="613" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/04/distro-terminal.png 613w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/distro-terminal-300x120.png 300w" sizes="(max-width: 613px) 100vw, 613px" /&gt;&lt;/p&gt; &lt;h2&gt;Configure AWS&lt;/h2&gt; &lt;p&gt;For this, follow the instructions at steps one and two on the &lt;a href="https://cloud.openshift.com/clusters/install"&gt;Red Hat OpenShift 4.0 on AWS installation page&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Install openshift-install&lt;/h2&gt; &lt;p&gt;The instructions from &lt;a href="https://try.openshift.com"&gt;try.openshift.com&lt;/a&gt; are browser-based. That is, you visit a web page and click on a button to download the bits. We&amp;#8217;re not going to use that. We&amp;#8217;re going to use &lt;code&gt;wget&lt;/code&gt; instead to download the bits from the command line.&lt;/p&gt; &lt;p&gt;Use your distro&amp;#8217;s package management tool to make sure &lt;code&gt;wget&lt;/code&gt; is installed. On my Fedora Remix distro, I had to run &lt;code&gt;sudo yum install -y wget&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Open your Windows browser of choice and visit the &lt;a href="https://github.com/openshift/installer/releases"&gt;openshift installer releases web page&lt;/a&gt;. Right mouse-click on the link for &amp;#8220;openshift-install-linux-amd64&amp;#8221; and select the option to copy the link to your local clipboard. Then switch over to your Linux terminal window so you can paste in the link and run the command:&lt;/p&gt; &lt;pre&gt;wget &amp;#60;paste-the-link-in-here&amp;#62;&lt;/pre&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-586757 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer-1024x49.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer.png" alt="" width="1327" height="63" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer.png 1327w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer-300x14.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer-768x36.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/wget-installer-1024x49.png 1024w" sizes="(max-width: 1327px) 100vw, 1327px" /&gt;&lt;/p&gt; &lt;p&gt;You&amp;#8217;ll see a few super-long URLs scroll by before you see the bits downloading to your Linux distro in your WSL.&lt;/p&gt; &lt;p&gt;Rename the downloaded bits with this command:&lt;/p&gt; &lt;pre&gt;mv openshift-install-linux-amd64 openshift-install&lt;/pre&gt; &lt;p&gt;Now, make it executable with this command:&lt;/p&gt; &lt;pre&gt;chmod +x openshift-install&lt;/pre&gt; &lt;h2&gt;Start the cluster&lt;/h2&gt; &lt;p&gt;Now you have the OpenShift installer available on your Linux distro. You can take the steps needed to start your Red Hat OpenShift 4.0 cluster on AWS. If you followed the &amp;#8220;Configure AWS&amp;#8221; steps above, you&amp;#8217;ll have the Access ID and key needed.&lt;/p&gt; &lt;p&gt;You can copy the Pull Secret from the &lt;a href="https://cloud.openshift.com/clusters/install"&gt;installation page&lt;/a&gt; mentioned earlier as well.&lt;/p&gt; &lt;p&gt;The magic starts to happen when you use the following command in your Linux terminal:&lt;/p&gt; &lt;pre&gt;./openshift-install create cluster&lt;/pre&gt; &lt;p&gt;Be patient. I&amp;#8217;ve had it take up to 24 minutes for a cluster to install. Don&amp;#8217;t worry; it&amp;#8217;s worth the wait.&lt;/p&gt; &lt;h2&gt;Install the OpenShift CLI for Windows&lt;/h2&gt; &lt;p&gt;When the installation is finished, your cluster will be up and running. You won&amp;#8217;t be able to log in until the time, but you can go ahead and make sure the OpenShift command-line tool, &lt;code&gt;oc&lt;/code&gt;, is installed on your Windows PC. At your PowerShell command line, run:&lt;/p&gt; &lt;pre&gt;choco install -y openshift-cli&lt;/pre&gt; &lt;p&gt;Again, if you don&amp;#8217;t have &lt;a href="https://chocolatey.org/"&gt;Chocolatey&lt;/a&gt; installed on your Windows machine, stop everything and get it immediately. It&amp;#8217;s that good.&lt;/p&gt; &lt;h2&gt;Copy the configuration file from WSL to Windows&lt;/h2&gt; &lt;p&gt;Here&amp;#8217;s the part where you make the transition from Linux to Windows, which will allow you to run everything from your PowerShell command line—and from inside VS Code too, if you wish.&lt;/p&gt; &lt;p&gt;After the installation is finished and the cluster is up and running, a configuration file will be written to your Linux distro&amp;#8217;s filesystem. The file is &lt;code&gt;/home/&amp;#60;your-username&amp;#62;/auth/kubeconfig&lt;/code&gt;. We need to copy this file to a location where we can read it from Windows PowerShell.&lt;/p&gt; &lt;p&gt;To date (April 2019), the WSL filesystem maps to the Windows filesystem as follows:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;/mnt/c&lt;/strong&gt; (Linux) maps to &lt;strong&gt;C:\&lt;/strong&gt; (Windows)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Future versions of Windows 10 will make it much easier to share data files between WSL and Windows. For now, just to be safe, we&amp;#8217;ll copy the configuration file.&lt;/p&gt; &lt;p&gt;So, we can create a directory—I did it off the root of my C: drive to keep it simple—and then use the Linux &lt;code&gt;cp&lt;/code&gt;(copy) command to get the config file over to Windows.&lt;/p&gt; &lt;p&gt;In PowerShell:&lt;/p&gt; &lt;pre&gt;mkdir C:\ocp4aws&lt;/pre&gt; &lt;p&gt;In Linux:&lt;/p&gt; &lt;pre&gt;sudo cp /home/&amp;#60;username&amp;#62;/auth/kubeconfig /mnt/c/ocp4aws&lt;/pre&gt; &lt;p&gt;Switch back to your PowerShell terminal and move the directory &lt;code&gt;C:\ocp4aws&lt;/code&gt;. A listing of the directory contents will show the file &lt;code&gt;kubeconfig&lt;/code&gt;, which is now available to your PowerShell session.&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-586717 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/04/ls-ocp4aws.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/04/ls-ocp4aws.png" alt="" width="479" height="165" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/04/ls-ocp4aws.png 479w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/ls-ocp4aws-300x103.png 300w" sizes="(max-width: 479px) 100vw, 479px" /&gt;&lt;/p&gt; &lt;p&gt;There&amp;#8217;s just one more step before logging in.&lt;/p&gt; &lt;p&gt;In PowerShell, set the environment variable &amp;#8220;KUBECONFIG&amp;#8221; that points to your config file:&lt;/p&gt; &lt;pre&gt;$env:KUBECONFIG="C:\ocp4aws\kubeconfig"&lt;/pre&gt; &lt;h2&gt;Stop. Take a Breath&lt;/h2&gt; &lt;p&gt;Up to this point, it&amp;#8217;s been a winding path with a lot of steps just to be able to log in to an OpenShift 4.0 cluster on AWS. Here&amp;#8217;s the good news: After this, you just need to do two steps: Set the &lt;code&gt;$env:KUBECONFIG&lt;/code&gt; environment variable and run &lt;code&gt;oc login...&lt;/code&gt;. That means, when you return to your PC tomorrow, you run two steps and, boom, you&amp;#8217;re connected to your cluster.&lt;/p&gt; &lt;p&gt;In other words:&lt;/p&gt; &lt;pre&gt;$env:KUBECONFIG="C:\ocp4aws\kubeconfig"&lt;/pre&gt; &lt;pre&gt;oc login -u kubeadmin -p &amp;#60;password-generated-during-cluster-install&amp;#62;&lt;/pre&gt; &lt;h2&gt;Log in from Windows&lt;/h2&gt; &lt;p&gt;Simply run the command as instructed in your Linux terminal session. It should look like the following—with a different password.&lt;/p&gt; &lt;pre&gt;oc login -u kubeadmin -p KSiVQ-gww7K-HEaiv-HNnfG&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Keep this information&lt;/strong&gt;. You&amp;#8217;ll need it to log into your cluster again.&lt;/p&gt; &lt;h2&gt;Next Steps&lt;/h2&gt; &lt;p&gt;To create or destroy a cluster, use the &lt;code&gt;openshift-install&lt;/code&gt; command within your Linux distro terminal.&lt;/p&gt; &lt;p&gt;After creating a &lt;em&gt;new&lt;/em&gt; cluster, you&amp;#8217;ll have a new login password. It&amp;#8217;ll be displayed in your Linux distro terminal after the cluster is created. Copy that to your clipboard to paste into PowerShell to log in from Windows.&lt;/p&gt; &lt;p&gt;Before you log in from Windows, make sure the &lt;code&gt;$env:KUBECONFIG&lt;/code&gt; is set.&lt;/p&gt; &lt;p&gt;That&amp;#8217;s it. You can now create, use and destroy your Red Hat OpenShift 4.0 cluster on AWS from your Windows PC.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#38;linkname=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fred-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows%2F&amp;#038;title=Red%20Hat%20OpenShift%204.0%20Developer%20Preview%20on%20AWS%3A%20Up%20and%20running%20with%20Windows" data-a2a-url="https://developers.redhat.com/blog/2019/04/23/red-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows/" data-a2a-title="Red Hat OpenShift 4.0 Developer Preview on AWS: Up and running with Windows"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/23/red-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows/"&gt;Red Hat OpenShift 4.0 Developer Preview on AWS: Up and running with Windows&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/i3rZ2OnLi4Y" height="1" width="1" alt=""/&gt;</content><summary>In a previous article, “OpenShift 4.0 Developer Preview on AWS is up and running” I included instructions for using macOS or Linux to install and manage your Red Hat OpenShift 4.0 cluster. Since I recently added a Windows 10 PC to my technology mix, I decided to try to use Windows as my only choice. I was saddened to learn that the installer, openshift-install, isn’t available for Windows. But, li...</summary><dc:creator>Don Schenck</dc:creator><dc:date>2019-04-23T07:03:07Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/23/red-hat-openshift-4-0-developer-preview-on-aws-up-and-running-with-windows/</feedburner:origLink></entry><entry><title>How to use the Linux perf tool to count software events</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YLSMpLynY2w/" /><category term="developer" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="gcc" scheme="searchisko:content:tags" /><category term="performance" scheme="searchisko:content:tags" /><author><name>William Cohen</name></author><id>searchisko:content:id:jbossorg_blog-how_to_use_the_linux_perf_tool_to_count_software_events</id><updated>2019-04-23T07:00:04Z</updated><published>2019-04-23T07:00:04Z</published><content type="html">&lt;p&gt;The Linux perf tool was originally written to allow access to the performance monitoring hardware that counts hardware events, such as instructions executed, processor cycles, and cache misses. However, it can also be used to count software events, which can be useful in gauging how frequently some part of the system software is executed.&lt;/p&gt; &lt;p&gt;Recently someone at Red Hat asked whether there was a way to get a count of system calls being executed on the system. The kernel has a predefined software trace point, &lt;code&gt;raw_syscalls:sys_enter&lt;/code&gt;, which collects that exact information; it counts each time a system call is made. To use the trace point events, the &lt;code&gt;perf&lt;/code&gt; command needs to be run as root.&lt;span id="more-585127"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The following code will give system-wide count (&lt;code&gt;-a&lt;/code&gt; option) of system calls (&lt;code&gt;-e raw_syscalls:sys_enter&lt;/code&gt;) every second (&lt;code&gt;-I 1000&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;# perf stat -a -e raw_syscalls:sys_enter -I 1000 # time counts unit events 1.000640941 1,250 raw_syscalls:sys_enter 2.001183785 1,901 raw_syscalls:sys_enter 3.001601593 1,922 raw_syscalls:sys_enter &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;raw_syscalls:sys_enter&lt;/code&gt; trace point is just one predefined trace point event in the kernel. To list the other 1000+ predefined trace points events, run the following as root:&lt;/p&gt; &lt;pre&gt;# perf list tracepoint List of pre-defined events (to be used in -e): block:block_bio_backmerge [Tracepoint event] block:block_bio_bounce [Tracepoint event] block:block_bio_complete [Tracepoint event] block:block_bio_frontmerge [Tracepoint event] block:block_bio_queue [Tracepoint event] block:block_bio_remap [Tracepoint event] block:block_dirty_buffer [Tracepoint event] block:block_getrq [Tracepoint event] block:block_plug [Tracepoint event] ... &lt;/pre&gt; &lt;p&gt;You may want to have a counter for some arbitrary function in the kernel that does not yet have a trace point. No problem. You can define your own probe points and then use them in the &lt;code&gt;perf stat&lt;/code&gt; command to monitor functions that implement expensive operations. For example, clearing a 2MB huge page has latency that is approximately 500 times longer than clearing a traditional 4KB page. These latencies can be noticeable, and you might want to know when a significant number of these delays occur.&lt;/p&gt; &lt;p&gt;The following sets up the probe point in the &lt;code&gt;clear_huge_page&lt;/code&gt; function accessible to perf:&lt;/p&gt; &lt;pre&gt;# perf probe --add clear_huge_page Added new event: probe:clear_huge_page (on clear_huge_page) You can now use it in all perf tools, such as: perf record -e probe:clear_huge_page -aR sleep 1 &lt;/pre&gt; &lt;p&gt;The following provides the count for every 10 seconds (10,000 milliseconds):&lt;/p&gt; &lt;pre&gt;# perf stat -a -e probe:clear_huge_page -I 10000 # time counts unit events 10.000241215 73 probe:clear_huge_page 20.001129381 4 probe:clear_huge_page 30.001567364 3 probe:clear_huge_page 40.002202895 2 probe:clear_huge_page 50.003554968 1 probe:clear_huge_page 50.316752807 0 probe:clear_huge_page ... &lt;/pre&gt; &lt;p&gt;When you no longer need the probe point for the &lt;code&gt;clear_huge_page&lt;/code&gt; function, it can be removed as shown below.&lt;/p&gt; &lt;pre&gt;# perf probe --del=probe:clear_huge_page Removed event: probe:clear_huge_page &lt;/pre&gt; &lt;p&gt;The perf probe points can also be placed user-space executables. You may need to compile the code with debuginfo enabled (GCC&amp;#8217;s &lt;code&gt;-g&lt;/code&gt; option) or to install the debuginfo RPMs to allow perf to find the location of the functions. To place a probe on the &lt;code&gt;malloc&lt;/code&gt; function in the glibc library, the executable needs to be specified with the &lt;code&gt;--exec&lt;/code&gt; option.&lt;/p&gt; &lt;pre&gt;# perf probe --exec=/lib64/libc-2.17.so --add malloc Added new event: probe_libc:malloc (on malloc in /usr/lib64/libc-2.17.so) You can now use it in all perf tools, such as: perf record -e probe_libc:malloc -aR sleep 1 &lt;/pre&gt; &lt;p&gt;Using &lt;code&gt;probe_libc:malloc&lt;/code&gt;, you can get a count of the number of &lt;code&gt;malloc&lt;/code&gt; calls occurring every 10 seconds. Below is the output from a machine that is initially sitting idle for the first 20 seconds. After 20 seconds, a parallel kernel build is started, and the number of times that malloc is called increases dramatically.&lt;/p&gt; &lt;pre&gt;# perf stat -a -e probe_libc:malloc -I 10000 # time counts unit events 10.000900150 2 probe_libc:malloc 20.001803180 0 probe_libc:malloc 30.002286255 1,829,385 probe_libc:malloc 40.002442647 12,553,306 probe_libc:malloc 50.002578104 15,579,692 probe_libc:malloc ... &lt;/pre&gt; &lt;p&gt;Once you&amp;#8217;re done with the user-space probe, it can be deleted:&lt;/p&gt; &lt;pre&gt;# perf probe --exec=/lib64/libc-2.17.so --del malloc Removed event: probe_libc:malloc &lt;/pre&gt; &lt;p&gt;Using &lt;code&gt;perf stat&lt;/code&gt; with the software probe points can help you answer the question of how frequently some code is being executed. For more information about setting up software probe points, take a look at the &lt;code&gt;perf-probe&lt;/code&gt; man page.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#38;linkname=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F23%2Fhow-to-use-the-linux-perf-tool-to-count-software-events%2F&amp;#038;title=How%20to%20use%20the%20Linux%20perf%20tool%20to%20count%20software%20events" data-a2a-url="https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/" data-a2a-title="How to use the Linux perf tool to count software events"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/"&gt;How to use the Linux perf tool to count software events&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YLSMpLynY2w" height="1" width="1" alt=""/&gt;</content><summary>The Linux perf tool was originally written to allow access to the performance monitoring hardware that counts hardware events, such as instructions executed, processor cycles, and cache misses. However, it can also be used to count software events, which can be useful in gauging how frequently some part of the system software is executed. Recently someone at Red Hat asked whether there was a way t...</summary><dc:creator>William Cohen</dc:creator><dc:date>2019-04-23T07:00:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/23/how-to-use-the-linux-perf-tool-to-count-software-events/</feedburner:origLink></entry><entry><title>How often do you contribute to open source projects?</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QNvtRhGq_Z8/" /><category term="community" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="poll" scheme="searchisko:content:tags" /><author><name>Rikki Endsley</name></author><id>searchisko:content:id:jbossorg_blog-how_often_do_you_contribute_to_open_source_projects</id><updated>2019-04-22T07:05:39Z</updated><published>2019-04-22T07:05:39Z</published><content type="html">&lt;p&gt;According to the most recent &lt;a href="https://insights.stackoverflow.com/survey/2019#developer-profile-_-contributing-to-open-source"&gt;Stack Overflow survey results&lt;/a&gt;, more than 36% of respondents &lt;em&gt;never&lt;/em&gt; contribute to open source projects. I wonder: Would we get different results in a Developer reader survey?&lt;/p&gt; &lt;p&gt;&lt;iframe src="https://docs.google.com/forms/d/e/1FAIpQLSd_nieyUbrVjGJ1Z9LDKWyS-IuM_Ks6Mh9JiQj6UDTgdMNuZA/viewform?embedded=true" width="640" height="541" frameborder="0" marginwidth="0" marginheight="0"&gt;Loading&amp;#8230;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;What readers said last week&lt;/h2&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/blog/2019/04/15/whats-your-biggest-work-environment-distraction/"&gt;last week&amp;#8217;s poll&lt;/a&gt;, we asked what your biggest workplace distractions are and almost 30% of respondents pointed to &amp;#8220;drive by&amp;#8221; interruptions from colleagues.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-medium wp-image-589437 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/04/Screen-Shot-2019-04-18-at-9.28.02-AM.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/04/Screen-Shot-2019-04-18-at-9.28.02-AM-300x159.png" alt="Survey results from last week in a pie chart" width="300" height="159" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/04/Screen-Shot-2019-04-18-at-9.28.02-AM-300x159.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/04/Screen-Shot-2019-04-18-at-9.28.02-AM.png 542w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#38;linkname=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fhow-often-do-you-contribute-to-open-source-projects%2F&amp;#038;title=How%20often%20do%20you%20contribute%20to%20open%20source%20projects%3F" data-a2a-url="https://developers.redhat.com/blog/2019/04/22/how-often-do-you-contribute-to-open-source-projects/" data-a2a-title="How often do you contribute to open source projects?"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/22/how-often-do-you-contribute-to-open-source-projects/"&gt;How often do you contribute to open source projects?&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QNvtRhGq_Z8" height="1" width="1" alt=""/&gt;</content><summary>According to the most recent Stack Overflow survey results, more than 36% of respondents never contribute to open source projects. I wonder: Would we get different results in a Developer reader survey? Loading… What readers said last week In last week’s poll, we asked what your biggest workplace distractions are and almost 30% of respondents pointed to “drive by” interruptions from colleagues.   T...</summary><dc:creator>Rikki Endsley</dc:creator><dc:date>2019-04-22T07:05:39Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/22/how-often-do-you-contribute-to-open-source-projects/</feedburner:origLink></entry><entry><title>Implicit function declarations: flex’s use of “reallocarray”</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hPEY7z7sH5w/" /><category term="community" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="fedora" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="gcc" scheme="searchisko:content:tags" /><category term="glibc" scheme="searchisko:content:tags" /><author><name>Arjun Shankar</name></author><id>searchisko:content:id:jbossorg_blog-implicit_function_declarations_flex_s_use_of_reallocarray</id><updated>2019-04-22T07:00:58Z</updated><published>2019-04-22T07:00:58Z</published><content type="html">&lt;p&gt;Several months ago, I took over the maintenance of the &lt;em&gt;flex&lt;/em&gt; package in &lt;a href="https://getfedora.org/"&gt;Fedora&lt;/a&gt; and decided to kick the tires by &lt;em&gt;rebasing&lt;/em&gt; the package in &lt;a href="https://fedoraproject.org/wiki/Releases/Rawhide"&gt;Fedora Rawhide&lt;/a&gt;. I downloaded and hashed the latest tarball at the time, &lt;em&gt;flex-2.6.4&lt;/em&gt;, tweaked the &lt;em&gt;spec&lt;/em&gt; file, and fired up a local build. Unfortunately, it failed with a &lt;code&gt;SIGSEGV&lt;/code&gt; at build time:&lt;/p&gt; &lt;pre&gt;./stage1flex -o stage1scan.c ./scan.l make[2]: *** [Makefile:1695: stage1scan.c] Segmentation fault (core dumped) &lt;/pre&gt; &lt;p&gt;Some debugging with &lt;em&gt;gdb&lt;/em&gt; led me to the conclusion that the segmentation fault was the result of a block of memory returned from the &lt;code&gt;reallocarray&lt;/code&gt; function being written to during flex initialization.  In this article, I&amp;#8217;ll describe the issue further and explain changes made to address it. &lt;span id="more-585327"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Here is a simplified snippet of my &lt;em&gt;gdb&lt;/em&gt; session:&lt;/p&gt; &lt;pre&gt;(gdb) bt #0 check_mul_overflow_size_t (right=1, left=2048, left@entry=0) #1 __GI___libc_reallocarray (optr=0x0, nmemb=2048, elem_size=1) #2 allocate_array at misc.c:147 #3 flexinit at main.c:974 #4 flex_main at main.c:168 #5 __libc_start_main (gdb) fin Run till exit from #0 check_mul_overflow_size_t __GI___libc_reallocarray 33 return realloc (optr, bytes); (gdb) fin Run till exit from #0 __GI___libc_reallocarray in allocate_array 147 mem = reallocarray(NULL, (size_t) size, element_size); Value returned is $1 = (void *) 0x5555557c6420 (gdb) fin Run till exit from #0 allocate_array in flexinit 974 action_array = allocate_character_array (action_size); Value returned is $2 = (void *) 0x557c6420 (gdb) n 975 defs1_offset = prolog_offset = action_offset = action_index = 0; (gdb) n 976 action_array[0] = '\0'; (gdb) n Program received signal SIGSEGV, Segmentation fault. &lt;/pre&gt; &lt;p&gt;I didn&amp;#8217;t notice anything off here right up to the point at which the segfault occurs, but maybe you already did. All &lt;em&gt;I&lt;/em&gt; saw was that the returned pointer was &lt;em&gt;non-NULL&lt;/em&gt; on line &lt;code&gt;974&lt;/code&gt;, but writing to it on line &lt;code&gt;976&lt;/code&gt; resulted in a segfault. It began to look like a &lt;code&gt;malloc&lt;/code&gt; bug.&lt;/p&gt; &lt;p&gt;On a whim, I built the same tarball outside of the Fedora build system. This time, the typical &lt;code&gt;./configure &amp;#38;&amp;#38; make&lt;/code&gt; command line didn&amp;#8217;t segfault at build time. So apparently the difference lay in the build options used by &lt;em&gt;rpmbuild&lt;/em&gt;. Some trial and error led me to the cause: &lt;code&gt;-pie&lt;/code&gt;, the linker flag that produces a position independent executable. Building with &lt;code&gt;-pie&lt;/code&gt; caused the segmentation fault.&lt;/p&gt; &lt;p&gt;Armed with this &amp;#8220;reproducer&amp;#8221; and advice from my colleagues at Red Hat, I set about doing a &lt;em&gt;git-bisect&lt;/em&gt; on the flex sources. &lt;em&gt;HEAD&lt;/em&gt; was building cleanly on the upstream &lt;em&gt;master&lt;/em&gt; branch at that point even with &lt;code&gt;-pie&lt;/code&gt;, so it was just a matter of finding the commit that fixed the build. The commit in question was the fix for the following issue reported against flex upstream:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/westes/flex/issues/241"&gt;#241: &amp;#8220;implicit declaration of function reallocarray is invalid in C99&amp;#8221;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, flex sources didn&amp;#8217;t declare &lt;code&gt;_GNU_SOURCE&lt;/code&gt;, leading to the compiler&amp;#8217;s seeing no declaration of the &lt;em&gt;reallocarray&lt;/em&gt; function. In such cases, the compiler creates an implicit function declaration with the default return type (&lt;code&gt;int&lt;/code&gt;) and generates code accordingly. On 64-bit Intel machines, the &lt;em&gt;int&lt;/em&gt; type is only 32 bits wide while pointers are 64 bits wide. Going back and looking at the gdb session, it then became clear to me that the pointer gets truncated:&lt;/p&gt; &lt;pre&gt;147 mem = reallocarray(NULL, (size_t) size, element_size); Value returned is $1 = (void *) 0x5555557c6420 (gdb) fin Run till exit from #0 allocate_array in flexinit 974 action_array = allocate_character_array (action_size); Value returned is $2 = (void *) 0x557c6420 &lt;/pre&gt; &lt;p&gt;This only happens in position independent executables because the heap gets mapped to a part of the address space where pointers are larger than &lt;code&gt;INT_MAX&lt;/code&gt;, exposing the above flex bug. GCC actually warns of the presence of implicit function declarations via the &lt;code&gt;-Wimplicit-function-declaration&lt;/code&gt; option. It appears that there was a fairly recent &lt;a href="https://fedoraproject.org/wiki/Changes/Fedora26CFlags"&gt;proposal to enable this warning&lt;/a&gt; in Fedora builds, but it was eventually shelved. If enabled, the warning would still cause the flex build to fail—but earlier and at a point where the problem was clear.&lt;/p&gt; &lt;p&gt;At this point, getting the build to compile successfully was a simple matter of backporting the corresponding flex patch that defines &lt;em&gt;_GNU_SOURCE&lt;/em&gt; and exposes the &lt;em&gt;reallocarray&lt;/em&gt; prototype to the compiler.&lt;/p&gt; &lt;p&gt;But we didn&amp;#8217;t just stop there. One of my colleagues, Florian Weimer—a regular contributor to glibc—thought that all this could have been avoided if &lt;em&gt;reallocarray&lt;/em&gt; had been exposed by glibc via the more general &lt;code&gt;_DEFAULT_SOURCE&lt;/code&gt; feature test macro. The change has now been &lt;a href="https://sourceware.org/git/?p=glibc.git;a=commit;h=2bda273aa3"&gt;committed&lt;/a&gt; to glibc upstream and is available since &lt;em&gt;glibc-2.29&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;With this change, we hope to avoid similar situations in other components in Fedora and the glibc user community. &lt;em&gt;glibc&lt;/em&gt; now provides the &lt;em&gt;reallocarray&lt;/em&gt; function prototype unless the user explicitly requires stricter conformance to a given standard.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#38;linkname=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F04%2F22%2Fimplicit-function-declarations-flexs-use-of-reallocarray%2F&amp;#038;title=Implicit%20function%20declarations%3A%20flex%E2%80%99s%20use%20of%20%E2%80%9Creallocarray%E2%80%9D" data-a2a-url="https://developers.redhat.com/blog/2019/04/22/implicit-function-declarations-flexs-use-of-reallocarray/" data-a2a-title="Implicit function declarations: flex’s use of “reallocarray”"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/04/22/implicit-function-declarations-flexs-use-of-reallocarray/"&gt;Implicit function declarations: flex&amp;#8217;s use of &amp;#8220;reallocarray&amp;#8221;&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hPEY7z7sH5w" height="1" width="1" alt=""/&gt;</content><summary>Several months ago, I took over the maintenance of the flex package in Fedora and decided to kick the tires by rebasing the package in Fedora Rawhide. I downloaded and hashed the latest tarball at the time, flex-2.6.4, tweaked the spec file, and fired up a local build. Unfortunately, it failed with a SIGSEGV at build time: ./stage1flex -o stage1scan.c ./scan.l make[2]: *** [Makefile:1695: stage1sc...</summary><dc:creator>Arjun Shankar</dc:creator><dc:date>2019-04-22T07:00:58Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/04/22/implicit-function-declarations-flexs-use-of-reallocarray/</feedburner:origLink></entry></feed>
